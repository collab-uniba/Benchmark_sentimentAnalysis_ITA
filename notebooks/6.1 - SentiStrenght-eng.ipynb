{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import translated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tw = pd.read_csv(\"../data/processed/Translated/twitter/dataset.csv\")\n",
    "sptr = pd.read_csv(\"../data/processed/Translated/SentiPolc/training_set_sentipolc16.csv\")\n",
    "\n",
    "#defining gold standard dataframes to get final results\n",
    "gs1 = pd.read_csv(\"../data/processed/Translated/twitter/dataset.csv\")\n",
    "gs2 = pd.read_csv(\"../data/processed/Translated/SentiPolc/training_set_sentipolc16.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enti Pubblici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Keep original tweets\n",
    "tw['tweetOrig_eng']=tw['eng']\n",
    "\n",
    "#Adding a space before and after each stopword in such a way not to consider the case in which the stopword is contained in a word\n",
    "words = set(stopwords.words('english'))\n",
    "stopwords = [' ' + x + ' ' for x in words]\n",
    "\n",
    "emoticons = [\"😀\",\"😃\",\"😄\",\"😁\",\"😆\",\"🤣\",\"😂\",\"🙂\",\"😊\",\"😍\",\"🥰\",\"🤩\",\"☺\",\"🥳\",\"😒\",\"😔\",\"😟\",\"🙁\",\"☹\",\"😥\",\"😢\",\"😭\",\"😱\",\"😞\",\"😓\",\"😩\",\"😫\",\"😡\",\"😠\",\"🤬\"]\n",
    "\n",
    "tw.eng = tw.eng.replace(\"@[\\w]*[_-]*[\\w]*\",\" \",regex=True)   # tag removal\n",
    "tw.eng = tw.eng.replace(\"https?://[\\w/%-.]*\",\" \",regex=True) # Url removal\n",
    "# Removing everything except the letters of the alphabet and the emoticons\n",
    "tw.tweet_x = tw.tweet_x.replace('[^ a-zA-Zà-ú'\n",
    "                            '\\😀\\😃\\😄\\😁\\😆\\🤣\\😂\\🙂\\😊\\😍\\🥰\\🤩\\☺\\🥳\\😒\\😔\\😟\\🙁\\☹\\😥\\😭\\😱\\😞\\😓\\😩\\😫\\😡\\😠\\🤬]', \" \",regex=True)    \n",
    "for word in emoticons:\n",
    "    tw.eng = tw.eng.replace(word, \" \"+word+\" \",regex=True) \n",
    "\n",
    "tw.eng = tw.eng.replace('\\s+', ' ',regex=True)               # Removal of excess spaces\n",
    "tw.eng = tw.eng.replace('^ ', '', regex=True)                # Removing the space at the beginning\n",
    "tw.eng = tw.eng.replace(' $', '', regex=True)                # Removing the space at the end\n",
    "tw.eng = tw.eng.apply(lambda x: x.lower())                   # Making everything in lowercase\n",
    "tw.eng = tw.eng.replace('^', ' ', regex=True) \n",
    "tw.eng = tw.eng.replace('$', ' ', regex=True)\n",
    "\n",
    "for word in stopwords:\n",
    "    tw.eng = tw.eng.replace(word, ' ',regex=True)\n",
    "\n",
    "# Removing the spaces at the beginning and at the end of every tweet\n",
    "tw.eng = tw.eng.apply(lambda x: x.strip())\n",
    "# Removing the empty tweets\n",
    "tw = tw[tw.eng != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentiPolc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Keep original tweets\n",
    "sptr['tweetOrig_eng']=sptr['eng']\n",
    "\n",
    "#Adding a space before and after each stopword in such a way not to consider the case in which the stopword is contained in a word\n",
    "words = set(stopwords.words('english'))\n",
    "stopwords = [' ' + x + ' ' for x in words]\n",
    "\n",
    "emoticons = [\"😀\",\"😃\",\"😄\",\"😁\",\"😆\",\"🤣\",\"😂\",\"🙂\",\"😊\",\"😍\",\"🥰\",\"🤩\",\"☺\",\"🥳\",\"😒\",\"😔\",\"😟\",\"🙁\",\"☹\",\"😥\",\"😢\",\"😭\",\"😱\",\"😞\",\"😓\",\"😩\",\"😫\",\"😡\",\"😠\",\"🤬\"]\n",
    "\n",
    "sptr.eng = sptr.eng.replace(\"@[\\w]*[_-]*[\\w]*\",\" \",regex=True)   # tag removal\n",
    "sptr.eng = sptr.eng.replace(\"https?://[\\w/%-.]*\",\" \",regex=True) # Url removal\n",
    "# Removing everything except the letters of the alphabet and the emoticons\n",
    "sptr.eng = sptr.eng.replace('[^ a-zA-Zà-ú'\n",
    "                            '\\😀\\😃\\😄\\😁\\😆\\🤣\\😂\\🙂\\😊\\😍\\🥰\\🤩\\☺\\🥳\\😒\\😔\\😟\\🙁\\☹\\😥\\😭\\😱\\😞\\😓\\😩\\😫\\😡\\😠\\🤬]', \" \",regex=True)    \n",
    "for word in emoticons:\n",
    "    sptr.eng = sptr.eng.replace(word, \" \"+word+\" \",regex=True) \n",
    "\n",
    "sptr.eng = sptr.eng.replace('\\s+', ' ',regex=True)               # Removal of excess spaces\n",
    "sptr.eng = sptr.eng.replace('^ ', '', regex=True)                # Removing the space at the beginning\n",
    "sptr.eng = sptr.eng.replace(' $', '', regex=True)                # Removing the space at the end\n",
    "sptr.eng = sptr.eng.apply(lambda x: x.lower())                   # Making everything in lowercase\n",
    "sptr.eng = sptr.eng.replace('^', ' ', regex=True) \n",
    "sptr.eng = sptr.eng.replace('$', ' ', regex=True)\n",
    "\n",
    "for word in stopwords:\n",
    "    sptr.eng = sptr.eng.replace(word, ' ',regex=True)\n",
    "\n",
    "# Removing the spaces at the beginning and at the end of every tweet\n",
    "sptr.eng = sptr.eng.apply(lambda x: x.strip())\n",
    "# Removing the empty tweets\n",
    "sptr = sptr[sptr.eng != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining list of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = tw['eng']\n",
    "sptr = sptr['eng']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert tweets to txt files to submit to SentiStrenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw.to_csv(\"../data/external/SentiStrenght/toClassify/translated_tw.txt\",header=None,index=None,sep=' ',mode='a')\n",
    "sptr.to_csv(\"../data/external/SentiStrenght/toClassify/translated_sptr.txt\",header=None,index=None,sep=' ',mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiStrenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "import os.path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentiStrengthLocation = \"C:/Users/polic/Desktop/Data Science/Tesi/Risorse/SentiStrength/SentiStrengthCom.jar\" # path for .jar file\n",
    "SentiStrengthLanguageFolder = \"../data/external/SentiStrenght/SentStrength_Data\" #path for directory containing SentiStrenght support files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test code\n",
    "if not os.path.isfile(SentiStrengthLocation):\n",
    "    print(\"SentiStrength not found at: \", SentiStrengthLocation)\n",
    "if not os.path.isdir(SentiStrengthLanguageFolder):\n",
    "    print(\"SentiStrength data folder not found at: \", SentiStrengthLanguageFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RateSentiment(sentiString):\n",
    "    #open a subprocess using shlex to get the command line string into the correct args list format\n",
    "    p = subprocess.Popen(shlex.split(\"java -jar '\" + SentiStrengthLocation + \"' stdin sentidata '\" + SentiStrengthLanguageFolder + \"'\"),stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "    #communicate via stdin the string to be rated. Note that all spaces are replaced with +\n",
    "    b = bytes(sentiString.replace(\" \",\"+\"), 'utf-8') #Can't send string in Python 3, must send bytes\n",
    "    stdout_byte, stderr_text = p.communicate(b)\n",
    "    stdout_text = stdout_byte.decode(\"utf-8\")  #convert from byte\n",
    "    stdout_text = stdout_text.rstrip().replace(\"/t\",\" \") #remove the tab spacing between the positive and negative ratings. e.g. 1    -5 -> 1 -5\n",
    "    return stdout_text + \" \" + sentiString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentiStrenght from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileToClassify = \"../data/external/SentiStrenght/toClassify/translated_tw.txt\"\n",
    "if not os.path.isfile(FileToClassify):\n",
    "    print(\"File to classify not found at: \", FileToClassify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running SentiStrength on file \" + FileToClassify + \" with command:\")\n",
    "cmd = 'java -jar \"' + SentiStrengthLocation + '\" sentidata \"' + SentiStrengthLanguageFolder + '\" input \"' + FileToClassify + '\"'\n",
    "print(cmd)\n",
    "p = subprocess.Popen(shlex.split(cmd),stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "classifiedSentimentFile = os.path.splitext(FileToClassify)[0] + \"0_out.txt\"\n",
    "print(\"Finished! The results will be in:/n\" + classifiedSentimentFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>new #bonus #inps coming! find get 1000 euros! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "      <td>so, let's recap; survivor's pension granted po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>dear dear accounts well, know worth ð? ð? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>photo - #napoli, opening #voragine, forty peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>maxi #protezionecivile tutorial. scenario bosc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Positive  Negative                                               Text\n",
       "0         2        -1  new #bonus #inps coming! find get 1000 euros! ...\n",
       "1         3        -2  so, let's recap; survivor's pension granted po...\n",
       "2         2        -1  dear dear accounts well, know worth ð? ð? ...\n",
       "3         1        -1  photo - #napoli, opening #voragine, forty peop...\n",
       "4         2        -1  maxi #protezionecivile tutorial. scenario bosc..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_out = pd.read_csv(\"../data/external/SentiStrenght/output/translated_tw0_out.txt\",sep='\\t',encoding='latin-1')\n",
    "tw_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset index to join dataframes\n",
    "tw_out=tw_out.reset_index()\n",
    "gs1=gs1.reset_index()\n",
    "#Mapping SentiStrenght scores to sentiment classes\n",
    "tw_out.loc[tw_out['Positive'] > 1, 'Pos_SS'] = 'yes'\n",
    "tw_out.loc[tw_out['Negative'] < -1, 'Neg_SS'] = 'yes'\n",
    "tw_out.loc[(tw_out['Positive'] <= 1)&(tw_out['Negative'] >= -1) , 'Neut_SS'] = 'yes'\n",
    "\n",
    "#Mapping GoldStandard scores to sentiment classes\n",
    "gs1.loc[gs1['Class'] =='pos', 'Pos_GS'] = 'yes'\n",
    "gs1.loc[gs1['Class'] == 'neg', 'Neg_GS'] = 'yes'\n",
    "gs1.loc[gs1['Class'] == 'neut', 'Neut_GS'] = 'yes'\n",
    "gs1.loc[gs1['Class'] == 'mix', 'Pos_GS'] = 'yes'\n",
    "gs1.loc[gs1['Class'] == 'mix', 'Neg_GS'] = 'yes'\n",
    "\n",
    "#Join dataframes\n",
    "val = gs1.merge(tw_out, how='inner', on='index')\n",
    "val = val.fillna(\"no\")\n",
    "val = val[['tweet_id','year','month','day','tweet_x','tweetOrig','Pos_GS','Pos_SS', 'Neg_GS','Neg_SS', 'Neut_GS','Neut_SS', 'Irony']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "### Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:  0.17493796526054592\n",
      "R:  0.734375\n",
      "F:  0.28256513026052105\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "print(\"P: \",precision_score(val['Pos_GS'],val['Pos_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(\"R: \",recall_score(val['Pos_GS'],val['Pos_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(\"F: \",f1_score(val['Pos_GS'],val['Pos_SS'],labels=['yes','no'],pos_label='yes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:  0.6052631578947368\n",
      "R:  0.4951560818083961\n",
      "F:  0.5447010065127293\n"
     ]
    }
   ],
   "source": [
    "print(\"P: \",precision_score(val['Neg_GS'],val['Neg_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(\"R: \",recall_score(val['Neg_GS'],val['Neg_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(\"F: \",f1_score(val['Neg_GS'],val['Neg_SS'],labels=['yes','no'],pos_label='yes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:  0.5387045813586098\n",
      "R:  0.42098765432098767\n",
      "F:  0.4726264726264726\n"
     ]
    }
   ],
   "source": [
    "print(\"P: \",precision_score(val['Neut_GS'],val['Neut_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(\"R: \",recall_score(val['Neut_GS'],val['Neut_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(\"F: \",f1_score(val['Neut_GS'],val['Neut_SS'],labels=['yes','no'],pos_label='yes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translated SentiPolc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileToClassify = \"../data/external/SentiStrenght/toClassify/translated_sptr.txt\"\n",
    "if not os.path.isfile(FileToClassify):\n",
    "    print(\"File to classify not found at: \", FileToClassify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running SentiStrength on file \" + FileToClassify + \" with command:\")\n",
    "cmd = 'java -jar \"' + SentiStrengthLocation + '\" sentidata \"' + SentiStrengthLanguageFolder + '\" input \"' + FileToClassify + '\"'\n",
    "print(cmd)\n",
    "p = subprocess.Popen(shlex.split(cmd),stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "classifiedSentimentFile = os.path.splitext(FileToClassify)[0] + \"0_out.txt\"\n",
    "print(\"Finished! The results will be in:/n\" + classifiedSentimentFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>meanwhile game via nazionale becomes complicat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>false illusions unpleasant realities mario mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>false illusions unpleasant realities editorial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>mario monti berlusconi spare italy blame causi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>mario monti berlusconi spare italy blame causi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Positive  Negative                                               Text\n",
       "0         1        -2  meanwhile game via nazionale becomes complicat...\n",
       "1         1        -3  false illusions unpleasant realities mario mon...\n",
       "2         1        -3  false illusions unpleasant realities editorial...\n",
       "3         1        -2  mario monti berlusconi spare italy blame causi...\n",
       "4         1        -2  mario monti berlusconi spare italy blame causi..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sptr_out = pd.read_csv(\"../data/external/SentiStrenght/output/translated_sptr0_out.txt\",sep='\\t',encoding='latin-1')\n",
    "sptr_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset index to join dataframes\n",
    "sptr_out=sptr_out.reset_index()\n",
    "gs2=gs2.reset_index()\n",
    "#Mapping SentiStrenght scores to sentiment classes\n",
    "sptr_out.loc[sptr_out['Positive'] > 1, 'Pos_SS'] = 'yes'\n",
    "sptr_out.loc[sptr_out['Negative'] < -1, 'Neg_SS'] = 'yes'\n",
    "sptr_out.loc[(sptr_out['Positive'] <= 1)&(sptr_out['Negative'] >= -1) , 'Neut_SS'] = 'yes'\n",
    "\n",
    "#Mapping SentiPolc scores to sentiment classes\n",
    "gs2.loc[gs2['opos'] ==1, 'Pos_GS'] = 'yes'\n",
    "gs2.loc[gs2['oneg'] ==1, 'Neg_GS'] = 'yes'\n",
    "gs2.loc[(gs2['opos'] ==0) & (gs2['oneg'] ==0), 'Neut_GS'] = 'yes'\n",
    "gs2 = gs2.fillna(\"no\")\n",
    "\n",
    "#Join dataframes\n",
    "val = gs2.merge(sptr_out, how='inner', on='index')\n",
    "val = val.fillna(\"no\")\n",
    "val = val[['idtwitter','text','eng','Pos_GS','Pos_SS', 'Neg_GS','Neg_SS', 'Neut_GS','Neut_SS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "### Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4090751645306547\n",
      "0.5772238514173998\n",
      "0.4788161362254207\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "print(precision_score(val['Pos_GS'],val['Pos_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(recall_score(val['Pos_GS'],val['Pos_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(f1_score(val['Pos_GS'],val['Pos_SS'],labels=['yes','no'],pos_label='yes'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5638562789043046\n",
      "0.5315224681421864\n",
      "0.547212152597963\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(val['Neg_GS'],val['Neg_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(recall_score(val['Neg_GS'],val['Neg_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(f1_score(val['Neg_GS'],val['Neg_SS'],labels=['yes','no'],pos_label='yes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5148176291793313\n",
      "0.4811789772727273\n",
      "0.49743024963289284\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(val['Neut_GS'],val['Neut_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(recall_score(val['Neut_GS'],val['Neut_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(f1_score(val['Neut_GS'],val['Neut_SS'],labels=['yes','no'],pos_label='yes'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03f8b8a1f797741164ec3397284a862f699c996acdec68272bf499e5af8d4843"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
