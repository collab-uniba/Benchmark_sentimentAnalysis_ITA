{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing SentiStrenght files from Custom Sentix Lexicon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentix_SS = pd.read_csv(\"../data/external/lexicon/sentix_ss.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IdiomLookupTable\n",
    "idiom = sentix_SS[(sentix_SS['Lemma'].str.contains('_'))==True]\n",
    "idiom[\"Lemma\"]= idiom[\"Lemma\"].str.replace(\"(\",\"\")\n",
    "idiom[\"Lemma\"]= idiom[\"Lemma\"].str.replace(\")\",\"\") \n",
    "\n",
    "#BoosterWordList\n",
    "bwl = [\"abbastanza\", \"sufficientemente\",\"bastantemente\",\"troppo\",\"poco\",\"pochino\",\"minimamente\",\"pochissimo\",\"meno\",\"alquanto\",\"piuttosto\",\"assai\",\"molto\",\"grandemente\",\"tanto\",\"massimamente\",\"affatto\",\"pi√π\",\"affatto\",\"proprio\",\"veramente\",\"assolutamente\",\"notevolmente\",\"particolarmente\",\"davvero\",\"super\",\"iper\",\"mega\",\"maxi\",\"completamente\"]\n",
    "boosterWordList = sentix_SS[sentix_SS['Lemma'].isin(bwl)]\n",
    "\n",
    "#NegatingWordList\n",
    "nwl = [\"non\",'nemmeno','neanche','neppure','no']\n",
    "negatingWordList = sentix_SS[sentix_SS['Lemma'].isin(nwl)]\n",
    "\n",
    "#EmotionLookupTable\n",
    "sentix_SS = pd.concat([sentix_SS,idiom,boosterWordList,negatingWordList]).drop_duplicates(keep=False,subset='New_Lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EmotionLookupTable\n",
    "elt = sentix_SS[['New_Lemma','Polarity SS']]\n",
    "elt['Polarity SS'] = elt['Polarity SS'].astype('int') \n",
    "elt.to_csv('../data/external/SentiStrenght/SentiStrenght_italian_Custom/EmotionLookupTable.txt', sep='\\t',header=None, index=None)\n",
    "\n",
    "#IdiomLookupTable\n",
    "ilt = idiom[[\"Lemma\",\"Polarity SS\"]]\n",
    "ilt['Polarity SS'] = ilt['Polarity SS'].astype('int') \n",
    "ilt.to_csv('../data/external/SentiStrenght/SentiStrenght_italian_Custom/IdiomLookupTable.txt', sep='\\t',header=None, index=None)\n",
    "\n",
    "#BoosterWordList\n",
    "bwl = boosterWordList[[\"New_Lemma\",\"Polarity SS\"]]\n",
    "bwl['Polarity SS'] = bwl['Polarity SS'].astype('int') \n",
    "bwl.to_csv('../data/external/SentiStrenght/SentiStrenght_italian_Custom/BoosterWordList.txt', sep='\\t',header=None, index=None)\n",
    "\n",
    "#NegatingWordList\n",
    "nwl = negatingWordList[[\"New_Lemma\"]]\n",
    "nwl.to_csv('../data/external/SentiStrenght/SentiStrenght_italian_Custom/NegatingWordList.txt',header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idiom handling\n",
    "idiom['Lemma_2'] = idiom['Lemma'].str.replace('_',' ')\n",
    "idList = idiom['Lemma_2'].to_list()\n",
    "idList2 = idiom['Lemma'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enti Pubblici Dataset\n",
    "## Preparing Gold Standard for SentiStrenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gs = pd.read_csv(\"../data/processed/Enti/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search and replace every idiomatic expression which is in list in tweets\n",
    "I=pd.Series(gs.tweet_x)\n",
    "for i in idList:\n",
    "    I.replace(i, i.replace(\" \",\"_\") ,regex=False, inplace = True)\n",
    "I=I.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing gold standard with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"it_core_news_lg\")\n",
    "I['spacy'] = I['tweet_x'].apply(lambda x: nlp(x))\n",
    "docs = I['spacy'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract lemmatization and POS tagging from individual spaCy doc\n",
    "def extract_lemma_pos(doc:spacy.tokens.doc.Doc):\n",
    "    return [token.lemma_+\"_\"+token.pos_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization and application of lemmatization and POS tagging\n",
    "def tidy_tokens(docs):\n",
    "    cols = [\"doc_id\",\"processed\"]\n",
    "    meta_df = []\n",
    "    for ix, doc in enumerate(docs):\n",
    "        meta = extract_lemma_pos(doc)\n",
    "        meta = pd.DataFrame(meta)\n",
    "        meta.columns = cols[1:]\n",
    "        meta = meta.assign(doc_id = ix).loc[:, cols]\n",
    "        meta_df.append(meta)\n",
    "        \n",
    "    return pd.concat(meta_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>in_ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>arrivo_NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>uno_DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>nuovo_ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>bonus_NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id    processed\n",
       "0       0       in_ADP\n",
       "1       0  arrivo_NOUN\n",
       "2       0      uno_DET\n",
       "3       0    nuovo_ADJ\n",
       "4       0   bonus_NOUN"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tidy_tokens(docs)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('doc_id')['processed'].apply(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [\"üòÄ\",\"üòÉ\",\"üòÑ\",\"üòÅ\",\"üòÜ\",\"ü§£\",\"üòÇ\",\"üôÇ\",\"üòä\",\"üòç\",\"ü•∞\",\"ü§©\",\"‚ò∫\",\"ü•≥\",\"üòí\",\"üòî\",\"üòü\",\"üôÅ\",\"‚òπ\",\"üò•\",\"üò¢\",\"üò≠\",\"üò±\",\"üòû\",\"üòì\",\"üò©\",\"üò´\",\"üò°\",\"üò†\",\"ü§¨\"]\n",
    "\n",
    "S=pd.Series(df.processed)\n",
    "\n",
    "#Regular expression to replace the POS tags with the lowercase version or to delete them in order to recognize terms in the support files (QuestionWords)\n",
    "for i in list:\n",
    "    S.replace(i+'\\S+', i,regex=True, inplace = True)\n",
    "\n",
    "for i in idList2:\n",
    "    S.replace(i+'\\S+', i,regex=True, inplace = True)\n",
    "\n",
    "S.replace(\"NOUN\",\"noun\",regex=True, inplace = True)\n",
    "S.replace(\"ADJ\",\"adj\",regex=True, inplace = True)\n",
    "S.replace(\"ADV\",\"adv\",regex=True, inplace = True)\n",
    "S.replace(\"VERB\",\"verb\",regex=True, inplace = True)\n",
    "S.replace(\"_ADP\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_AUX\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_CONJ\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_CCONJ\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_DET\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_INTJ\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_NUM\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_PART\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_PRON\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_PROPN\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_PUNCT\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_SCONJ\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_SYM\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_X\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_SPACE\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"[_][+]\\s\",\" \",regex=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'allora riepilogare_verb al politico_noun il pensione_noun di reversibilit√†_noun venire concesso_verb fino_adv al pronipote_noun fino_adv alla generazione_noun che non_adv lo prendere_verb uno colpo_noun alla povero_adj gente_noun inps dopo uno certo_adj et√†_noun togliere_verb il pensione_noun di invalidita ccivostra üò°'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert tweets to txt files to submit to SentiStrenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.to_csv(\"../data/external/SentiStrenght/toClassify/file.txt\",header=None,index=None,sep=' ',mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentipolc Dataset\n",
    "## Preparing for SentiStrenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = pd.read_csv(\"../data/processed/SentiPolc/test_set_sentipolc16_gold2000.csv\")\n",
    "\n",
    "#defining gold standard dataframes to get final results\n",
    "gs2 = pd.read_csv(\"../data/processed/SentiPolc/test_set_sentipolc16_gold2000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Keep original tweets\n",
    "sp['tweetOrig']=sp['text']\n",
    "\n",
    "#Adding a space before and after each stopword in such a way not to consider the case in which the stopword is contained in a word\n",
    "words = set(stopwords.words('italian'))\n",
    "stopwords = [' ' + x + ' ' for x in words]\n",
    "\n",
    "emoticons = [\"üòÄ\",\"üòÉ\",\"üòÑ\",\"üòÅ\",\"üòÜ\",\"ü§£\",\"üòÇ\",\"üôÇ\",\"üòä\",\"üòç\",\"ü•∞\",\"ü§©\",\"‚ò∫\",\"ü•≥\",\"üòí\",\"üòî\",\"üòü\",\"üôÅ\",\"‚òπ\",\"üò•\",\"üò¢\",\"üò≠\",\"üò±\",\"üòû\",\"üòì\",\"üò©\",\"üò´\",\"üò°\",\"üò†\",\"ü§¨\"]\n",
    "\n",
    "sp.text = sp.text.replace(\"@[\\w]*[_-]*[\\w]*\",\" \",regex=True)   # tag removal\n",
    "sp.text = sp.text.replace(\"https?://[\\w/%-.]*\",\" \",regex=True) # Url removal\n",
    "# Removing everything except the letters of the alphabet and the emoticons\n",
    "sp.text = sp.text.replace('[^ a-zA-Z√†-√∫'\n",
    "                            '\\üòÄ\\üòÉ\\üòÑ\\üòÅ\\üòÜ\\ü§£\\üòÇ\\üôÇ\\üòä\\üòç\\ü•∞\\ü§©\\‚ò∫\\ü•≥\\üòí\\üòî\\üòü\\üôÅ\\‚òπ\\üò•\\üò≠\\üò±\\üòû\\üòì\\üò©\\üò´\\üò°\\üò†\\ü§¨]', \" \",regex=True)    \n",
    "for word in emoticons:\n",
    "    sp.text = sp.text.replace(word, \" \"+word+\" \",regex=True) \n",
    "\n",
    "sp.text = sp.text.replace('\\s+', ' ',regex=True)               # Removal of excess spaces\n",
    "sp.text = sp.text.replace('^ ', '', regex=True)                # Removing the space at the beginning\n",
    "sp.text = sp.text.replace(' $', '', regex=True)                # Removing the space at the end\n",
    "sp.text = sp.text.apply(lambda x: x.lower())                   # Making everything in lowercase\n",
    "sp.text = sp.text.replace('^', ' ', regex=True) \n",
    "sp.text = sp.text.replace('$', ' ', regex=True)\n",
    "\n",
    "for word in stopwords:\n",
    "    sp.text = sp.text.replace(word, ' ',regex=True)\n",
    "\n",
    "# Removing the spaces at the beginning and at the end of every tweet\n",
    "sp.text = sp.text.apply(lambda x: x.strip())\n",
    "# Removing the empty tweets\n",
    "sp = sp[sp.text != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search and replace every idiomatic expression which is in list in tweets\n",
    "I=pd.Series(sp.text)\n",
    "for i in idList:\n",
    "    I.replace(i, i.replace(\" \",\"_\") ,regex=False, inplace = True)\n",
    "I=I.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"it_core_news_lg\")\n",
    "I['spacy'] = I['text'].apply(lambda x: nlp(x))\n",
    "docs = I['spacy'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>intanto_ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>il_DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>partita_NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>per_ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>via_NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id     processed\n",
       "0       0   intanto_ADV\n",
       "1       0        il_DET\n",
       "2       0  partita_NOUN\n",
       "3       0       per_ADP\n",
       "4       0      via_NOUN"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tidy_tokens(docs)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('doc_id')['processed'].apply(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [\"üòÄ\",\"üòÉ\",\"üòÑ\",\"üòÅ\",\"üòÜ\",\"ü§£\",\"üòÇ\",\"üôÇ\",\"üòä\",\"üòç\",\"ü•∞\",\"ü§©\",\"‚ò∫\",\"ü•≥\",\"üòí\",\"üòî\",\"üòü\",\"üôÅ\",\"‚òπ\",\"üò•\",\"üò¢\",\"üò≠\",\"üò±\",\"üòû\",\"üòì\",\"üò©\",\"üò´\",\"üò°\",\"üò†\",\"ü§¨\"]\n",
    "\n",
    "S=pd.Series(df.processed)\n",
    "\n",
    "#Regular expression to replace the POS tags with the lowercase version or to delete them in order to recognize terms in the support files (QuestionWords)\n",
    "for i in list:\n",
    "    S.replace(i+'\\S+', i,regex=True, inplace = True)\n",
    "\n",
    "for i in idList2:\n",
    "    S.replace(i+'\\S+', i,regex=True, inplace = True)\n",
    "\n",
    "S.replace(\"NOUN\",\"noun\",regex=True, inplace = True)\n",
    "S.replace(\"ADJ\",\"adj\",regex=True, inplace = True)\n",
    "S.replace(\"ADV\",\"adv\",regex=True, inplace = True)\n",
    "S.replace(\"VERB\",\"verb\",regex=True, inplace = True)\n",
    "S.replace(\"_ADP\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_AUX\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_CONJ\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_CCONJ\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_DET\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_INTJ\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_NUM\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_PART\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_PRON\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_PROPN\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_PUNCT\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_SCONJ\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_SYM\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_X\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"_SPACE\",\"\",regex=True, inplace = True)\n",
    "S.replace(\"[_][+]\\s\",\" \",regex=True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert tweets to txt files to submit to SentiStrenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.to_csv(\"../data/external/SentiStrenght/toClassify/sp.txt\",header=None,index=None,sep=' ',mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentiStrenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "import os.path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentiStrengthLocation = \"../SentiStrengthCom.jar\" # path for .jar file\n",
    "SentiStrengthLanguageFolder = \"../data/external/SentiStrenght/SentiStrength_Italian_Custom/\" #path for directory containing SentiStrenght support files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test code\n",
    "if not os.path.isfile(SentiStrengthLocation):\n",
    "    print(\"SentiStrength not found at: \", SentiStrengthLocation)\n",
    "if not os.path.isdir(SentiStrengthLanguageFolder):\n",
    "    print(\"SentiStrength data folder not found at: \", SentiStrengthLanguageFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RateSentiment(sentiString):\n",
    "    #open a subprocess using shlex to get the command line string into the correct args list format\n",
    "    p = subprocess.Popen(shlex.split(\"java -jar '\" + SentiStrengthLocation + \"' stdin sentidata '\" + SentiStrengthLanguageFolder + \"'\"),stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "    #communicate via stdin the string to be rated. Note that all spaces are replaced with +\n",
    "    b = bytes(sentiString.replace(\" \",\"+\"), 'utf-8') #Can't send string in Python 3, must send bytes\n",
    "    stdout_byte, stderr_text = p.communicate(b)\n",
    "    stdout_text = stdout_byte.decode(\"utf-8\")  #convert from byte\n",
    "    stdout_text = stdout_text.rstrip().replace(\"/t\",\" \") #remove the tab spacing between the positive and negative ratings. e.g. 1    -5 -> 1 -5\n",
    "    return stdout_text + \" \" + sentiString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentiStrenght from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enti Pubblici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileToClassify = \"../data/external/SentiStrenght/toClassify/file.txt\"\n",
    "if not os.path.isfile(FileToClassify):\n",
    "    print(\"File to classify not found at: \", FileToClassify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running SentiStrength on file \" + FileToClassify + \" with command:\")\n",
    "cmd = 'java -jar \"' + SentiStrengthLocation + '\" sentidata \"' + SentiStrengthLanguageFolder + '\" input \"' + FileToClassify + '\"'\n",
    "print(cmd)\n",
    "p = subprocess.Popen(shlex.split(cmd),stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "classifiedSentimentFile = os.path.splitext(FileToClassify)[0] + \"0_out.txt\"\n",
    "print(\"Finished! The results will be in:/n\" + classifiedSentimentFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>-4</td>\n",
       "      <td>in arrivo_noun uno nuovo_adj bonus_noun inps s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-4</td>\n",
       "      <td>allora_adv riepilogare_verb al politico_noun i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>caro_adj e caro_adj fare_verb bene_adv il vost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>foto_noun a napoli dopo l apertura_noun di uno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "      <td>maxi_adj esercitazione_noun di protezionecivil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Positive  Negative                                               Text\n",
       "0         2        -4  in arrivo_noun uno nuovo_adj bonus_noun inps s...\n",
       "1         2        -4  allora_adv riepilogare_verb al politico_noun i...\n",
       "2         4        -1  caro_adj e caro_adj fare_verb bene_adv il vost...\n",
       "3         2        -1  foto_noun a napoli dopo l apertura_noun di uno...\n",
       "4         3        -2  maxi_adj esercitazione_noun di protezionecivil..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = pd.read_csv(\"../data/external/SentiStrenght/output/file1_out.txt\",sep='\\t',encoding='latin-1')\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset index to join dataframes\n",
    "comments=comments.reset_index()\n",
    "gs=gs.reset_index()\n",
    "#Mapping SentiStrenght scores to sentiment classes\n",
    "comments.loc[comments['Positive'] > 1, 'Pos_SS'] = 'yes'\n",
    "comments.loc[comments['Negative'] < -1, 'Neg_SS'] = 'yes'\n",
    "comments.loc[(comments['Positive'] <= 1)&(comments['Negative'] >= -1) , 'Neut_SS'] = 'yes'\n",
    "\n",
    "#Mapping GoldStandard scores to sentiment classes\n",
    "gs.loc[gs['Class'] =='pos', 'Pos_GS'] = 'yes'\n",
    "gs.loc[gs['Class'] == 'neg', 'Neg_GS'] = 'yes'\n",
    "gs.loc[gs['Class'] == 'neut', 'Neut_GS'] = 'yes'\n",
    "gs.loc[gs['Class'] == 'mix', 'Pos_GS'] = 'yes'\n",
    "gs.loc[gs['Class'] == 'mix', 'Neg_GS'] = 'yes'\n",
    "\n",
    "#Join dataframes\n",
    "val = gs.merge(comments, how='inner', on='index')\n",
    "val = val.fillna(\"no\")\n",
    "val = val[['tweet_id','year','month','day','tweet_x','tweetOrig','Pos_GS','Pos_SS', 'Neg_GS','Neg_SS', 'Neut_GS','Neut_SS', 'Irony']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>tweet_x</th>\n",
       "      <th>tweetOrig</th>\n",
       "      <th>Pos_GS</th>\n",
       "      <th>Pos_SS</th>\n",
       "      <th>Neg_GS</th>\n",
       "      <th>Neg_SS</th>\n",
       "      <th>Neut_GS</th>\n",
       "      <th>Neut_SS</th>\n",
       "      <th>Irony</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1462002288835403777</td>\n",
       "      <td>2021</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>in arrivo un nuovo bonus inps scopri chi pu√≤ o...</td>\n",
       "      <td>In arrivo un nuovo #bonus #inps! Scopri chi pu...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1354381987507744771</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>allora riepiloghiamo ai politici la pensione d...</td>\n",
       "      <td>Allora, riepiloghiamo;\\nAi politici la pension...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1454050817821003783</td>\n",
       "      <td>2021</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "      <td>caro e cari fate bene i vostri conti perch√© no...</td>\n",
       "      <td>Caro @INPS_it e cari @Europarl_IT  fate bene i...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1393675898960982016</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>foto a napoli dopo l apertura di una voragine ...</td>\n",
       "      <td>FOTO - A #napoli, dopo l'apertura di una #vora...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1417876270705164289</td>\n",
       "      <td>2021</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>maxi esercitazione di protezionecivile lo scen...</td>\n",
       "      <td>Maxi esercitazione di #protezionecivile. Lo sc...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id  year  month  day  \\\n",
       "0  1462002288835403777  2021     11   20   \n",
       "1  1354381987507744771  2021      1   27   \n",
       "2  1454050817821003783  2021     10   29   \n",
       "3  1393675898960982016  2021      5   15   \n",
       "4  1417876270705164289  2021      7   21   \n",
       "\n",
       "                                             tweet_x  \\\n",
       "0  in arrivo un nuovo bonus inps scopri chi pu√≤ o...   \n",
       "1  allora riepiloghiamo ai politici la pensione d...   \n",
       "2  caro e cari fate bene i vostri conti perch√© no...   \n",
       "3  foto a napoli dopo l apertura di una voragine ...   \n",
       "4  maxi esercitazione di protezionecivile lo scen...   \n",
       "\n",
       "                                           tweetOrig Pos_GS Pos_SS Neg_GS  \\\n",
       "0  In arrivo un nuovo #bonus #inps! Scopri chi pu...    yes    yes     no   \n",
       "1  Allora, riepiloghiamo;\\nAi politici la pension...     no    yes    yes   \n",
       "2  Caro @INPS_it e cari @Europarl_IT  fate bene i...     no    yes    yes   \n",
       "3  FOTO - A #napoli, dopo l'apertura di una #vora...     no    yes     no   \n",
       "4  Maxi esercitazione di #protezionecivile. Lo sc...     no    yes     no   \n",
       "\n",
       "  Neg_SS Neut_GS Neut_SS Irony  \n",
       "0    yes      no      no    no  \n",
       "1    yes      no      no   yes  \n",
       "2     no      no      no   yes  \n",
       "3     no     yes      no    no  \n",
       "4    yes     yes      no    no  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "#### Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:  0.10303752233472305\n",
      "R:  0.9010416666666666\n",
      "F:  0.18492784607161947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "print(\"P: \",precision_score(val['Pos_GS'],val['Pos_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(\"R: \",recall_score(val['Pos_GS'],val['Pos_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(\"F: \",f1_score(val['Pos_GS'],val['Pos_SS'],labels=['yes','no'],pos_label='yes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:  0.5202231520223152\n",
      "R:  0.8030139935414424\n",
      "F:  0.6314007617435462\n"
     ]
    }
   ],
   "source": [
    "print(\"P: \",precision_score(val['Neg_GS'],val['Neg_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(\"R: \",recall_score(val['Neg_GS'],val['Neg_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(\"F: \",f1_score(val['Neg_GS'],val['Neg_SS'],labels=['yes','no'],pos_label='yes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:  0.4666666666666667\n",
      "R:  0.043209876543209874\n",
      "F:  0.07909604519774012\n"
     ]
    }
   ],
   "source": [
    "print(\"P: \",precision_score(val['Neut_GS'],val['Neut_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(\"R: \",recall_score(val['Neut_GS'],val['Neut_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(\"F: \",f1_score(val['Neut_GS'],val['Neut_SS'],labels=['yes','no'],pos_label='yes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentiPolc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileToClassify = \"../data/external/SentiStrenght/toClassify/sp.txt\"\n",
    "if not os.path.isfile(FileToClassify):\n",
    "    print(\"File to classify not found at: \", FileToClassify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running SentiStrength on file \" + FileToClassify + \" with command:\")\n",
    "cmd = 'java -jar \"' + SentiStrengthLocation + '\" sentidata \"' + SentiStrengthLanguageFolder + '\" input \"' + FileToClassify + '\"'\n",
    "print(cmd)\n",
    "p = subprocess.Popen(shlex.split(cmd),stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "classifiedSentimentFile = os.path.splitext(FileToClassify)[0] + \"0_out.txt\"\n",
    "print(\"Finished! The results will be in:/n\" + classifiedSentimentFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>intanto_adv partita via nazionale_adj complica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-5</td>\n",
       "      <td>falso_adj illusione_noun sgradevole_adj realt√É...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-5</td>\n",
       "      <td>falso_adj illusione_noun sgradevole_adj realt√É...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>-3</td>\n",
       "      <td>mario monte berlusconi risparmio_noun italia b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-3</td>\n",
       "      <td>mario monte berlusconi risparmio_noun italia b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Positive  Negative                                               Text\n",
       "0         1        -2  intanto_adv partita via nazionale_adj complica...\n",
       "1         1        -5  falso_adj illusione_noun sgradevole_adj realt√É...\n",
       "2         1        -5  falso_adj illusione_noun sgradevole_adj realt√É...\n",
       "3         2        -3  mario monte berlusconi risparmio_noun italia b...\n",
       "4         2        -3  mario monte berlusconi risparmio_noun italia b..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_out = pd.read_csv(\"../data/external/SentiStrenght/output/sp0_out.txt\",sep='\\t',encoding='latin-1')\n",
    "sp_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset index to join dataframes\n",
    "sp_out=sp_out.reset_index()\n",
    "gs2=gs2.reset_index()\n",
    "#Mapping SentiStrenght scores to sentiment classes\n",
    "sp_out.loc[sp_out['Positive'] > 1, 'Pos_SS'] = 'yes'\n",
    "sp_out.loc[sp_out['Negative'] < -1, 'Neg_SS'] = 'yes'\n",
    "sp_out.loc[(sp_out['Positive'] <= 1)&(sp_out['Negative'] >= -1) , 'Neut_SS'] = 'yes'\n",
    "\n",
    "#Mapping SentiPolc scores to sentiment classes\n",
    "gs2.loc[gs2['opos'] ==1, 'Pos_GS'] = 'yes'\n",
    "gs2.loc[gs2['oneg'] ==1, 'Neg_GS'] = 'yes'\n",
    "gs2.loc[(gs2['opos'] ==0) & (gs2['oneg'] ==0), 'Neut_GS'] = 'yes'\n",
    "gs2 = gs2.fillna(\"no\")\n",
    "\n",
    "#Join dataframes\n",
    "val = gs2.merge(sp_out, how='inner', on='index')\n",
    "val = val.fillna(\"no\")\n",
    "val = val[['idtwitter','text','Pos_GS','Pos_SS', 'Neg_GS','Neg_SS', 'Neut_GS','Neut_SS','iro']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29413912725944863\n",
      "0.7870053737176356\n",
      "0.4282296650717704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "print(precision_score(val['Pos_GS'],val['Pos_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(recall_score(val['Pos_GS'],val['Pos_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(f1_score(val['Pos_GS'],val['Pos_SS'],labels=['yes','no'],pos_label='yes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4428145201392342\n",
      "0.5972501676727029\n",
      "0.5085665334094803\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(val['Neg_GS'],val['Neg_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(recall_score(val['Neg_GS'],val['Neg_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(f1_score(val['Neg_GS'],val['Neg_SS'],labels=['yes','no'],pos_label='yes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4575757575757576\n",
      "0.16086647727272727\n",
      "0.23804519180241726\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(val['Neut_GS'],val['Neut_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(recall_score(val['Neut_GS'],val['Neut_SS'],labels=['yes','no'],pos_label='yes'))\n",
    "print(f1_score(val['Neut_GS'],val['Neut_SS'],labels=['yes','no'],pos_label='yes'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03f8b8a1f797741164ec3397284a862f699c996acdec68272bf499e5af8d4843"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
